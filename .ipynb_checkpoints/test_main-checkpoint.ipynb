{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "801aa11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import cv2\n",
    "\n",
    "from src.Node import Node\n",
    "from src.NCMTree import NCMTree\n",
    "from src.NCMForest import NCMForest\n",
    "from headers.NCMClassifier import NCMClassifier\n",
    "from headers.OneCentroid import OneCentroid\n",
    "from headers.utils import *\n",
    "\n",
    "def test_gen_Node():\n",
    "    #fit and plot a generative node\n",
    "    digits=load_digits()\n",
    "    X = digits.data\n",
    "    y = digits.target\n",
    "    for i in range(len(np.unique(y))):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "        Norm = Normalizer()\n",
    "        X_train = Norm.fit_transform(X_train)\n",
    "        X_test = Norm.transform(X_test)\n",
    "        print(\"Node with root class = \", i)\n",
    "        node = Node(None, False, 1, 21, method_split='generative', distance=\"euclidean\", root_class=i, alpha=0.99)\n",
    "        node.fit(X_train, y_train)\n",
    "        print(\"Train set split : \")\n",
    "        node.plot(X_train)\n",
    "        print(\"Test set performance : \")\n",
    "        class_index_test = np.where(np.isin(y_test, i))\n",
    "        y_test[y_test > 0] = 0\n",
    "        y_test[class_index_test] = 1\n",
    "        pred = node.splitting_clf.predict(X_test)\n",
    "        print(\"Test Accuracy : \", np.mean(pred==y_test))\n",
    "        confmat = confusion_matrix(y_test, pred)\n",
    "        print(\"Confusion Matrix : \")\n",
    "        print(confmat)\n",
    "\n",
    "\n",
    "def test_forest():        \n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train=X_train.reshape(X_train.shape[0], 784)\n",
    "    X_test=X_test.reshape(X_test.shape[0], 784)\n",
    "    \n",
    "    clf = NCMForest(n_classes=10, tree_multiplier = 1, max_depth=20, min_samples_split=2, min_samples_leaf=5,\n",
    "                         method_max_features=\"sqrt\", method_split='maj_class',\n",
    "                       distance=\"euclidean\", root_distance = \"mahalanobis\", alpha=0.95, nbgenlayer=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # print the classification report\n",
    "    print(classification_report(y_test, clf.predict(X_test), digits=5))\n",
    "    \n",
    "\n",
    "def deskew(img, imgSize):\n",
    "    # calculate image moments\n",
    "    m = cv2.moments(img)\n",
    "    if abs(m['mu02']) < 1e-2:\n",
    "        # no deskewing needed\n",
    "        return img.copy()\n",
    "\n",
    "    # calculate skew based on central moments\n",
    "    skew = m['mu11'] / m['mu02']\n",
    "\n",
    "    # calculate affine transformation to correct skewness\n",
    "    M = np.float32([[1, skew, -0.5*imgSize*skew], [0, 1, 0]])\n",
    "\n",
    "    # apply affine transformation\n",
    "    img = cv2.warpAffine(img, M, (imgSize, imgSize), flags=cv2.WARP_INVERSE_MAP | cv2.INTER_LINEAR)\n",
    "\n",
    "    return img\n",
    "\n",
    "def HOG_descriptors():\n",
    "    # Load the mnist dataset\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \"\"\"\n",
    "    #Block made to remove certain classes from the training and testing set, for testing on partial dataset\n",
    "    #Enter classes you wish to remove into the list below\n",
    "    classtodelete=[]\n",
    "    \n",
    "    class_to_delete_index_train = np.where(np.isin(y_train, classtodelete))\n",
    "    class_to_delete_index_test = np.where(np.isin(y_test, classtodelete))\n",
    "    \n",
    "    X_train = np.delete(X_train, class_to_delete_index_train, 0)\n",
    "    y_train = np.delete(y_train, class_to_delete_index_train)\n",
    "    X_test = np.delete(X_test, class_to_delete_index_test, 0)\n",
    "    y_test = np.delete(y_test, class_to_delete_index_test)\n",
    "    \"\"\"\n",
    "    imsize = 28 # size of image (28x28)\n",
    "\n",
    "    # HOG parameters:\n",
    "    winSize = (imsize, imsize) # 28, 28\n",
    "    blockSize = (imsize//2, imsize//2) # 14, 14    \n",
    "    cellSize = (imsize//2, imsize//2) #14, 14\n",
    "    blockStride = (imsize//4, imsize//4) # 7, 7\n",
    "    nbins = 9\n",
    "    signedGradients = True\n",
    "    derivAperture = 1\n",
    "    winSigma = -1.0\n",
    "    histogramNormType = 0\n",
    "    L2HysThreshold = 0.2\n",
    "    gammaCorrection = 1\n",
    "    nlevels = 64\n",
    "\n",
    "    # define the HOG descriptor\n",
    "    hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins, derivAperture, winSigma, \n",
    "                        histogramNormType, L2HysThreshold, gammaCorrection, nlevels, signedGradients)\n",
    "\n",
    "    # compute HOG descriptors\n",
    "    train_descriptors = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        X_train[i] = deskew(X_train[i], 28) # deskew the current image\n",
    "        descriptor = hog.compute(X_train[i]) # compute the HOG features\n",
    "        train_descriptors.append(descriptor) # append it to the train decriptors list\n",
    "    \n",
    "    test_descriptors = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        X_test[i] = deskew(X_test[i], 28) # deskew the current image\n",
    "        descriptor = hog.compute(X_test[i]) # compute the HOG features\n",
    "        test_descriptors.append(descriptor) # append it to the test descriptors list\n",
    "    \n",
    "    #train_descriptors = np.array(train_descriptors)\n",
    "    train_descriptors = np.resize(train_descriptors, (X_train.shape[0], 81))\n",
    "    \n",
    "    #test_descriptors = np.array(test_descriptors)\n",
    "    test_descriptors = np.resize(test_descriptors, (X_test.shape[0], 81))\n",
    "\n",
    "    return train_descriptors, test_descriptors, y_train, y_test\n",
    "\n",
    "def HOG_test():\n",
    "    #Build and test forest with HOG descriptors\n",
    "    X_train, X_test, y_train, y_test = HOG_descriptors()\n",
    "    # classifier\n",
    "    clf = NCMForest(n_classes=10, tree_multiplier = 1, max_depth=20, min_samples_split=2, min_samples_leaf=5,\n",
    "                         method_max_features=\"sqrt\", method_split='maj_class',\n",
    "                       distance=\"euclidean\", root_distance = \"mahalanobis\", alpha=0.95, nbgenlayer=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # print the classification report\n",
    "    print(classification_report(y_test, clf.predict(X_test), digits=5))\n",
    "\n",
    "def test_newclass():\n",
    "    #This tests whether or not a potential new class can be detected by training the roots on 9/10 (or 8/10, 7/10 etc...) classes and checking if the samples that weren't part of the training set are rejected by all roots.\n",
    "    newclass = 10 #newclass = 10 will set it so all classes are present, used to test class recognition, to test new class detection set newclass as any from 0-9\n",
    "    X_train, X_test, y_train, y_test = HOG_descriptors()\n",
    "    #printing the shapes of the vectors \n",
    "    print('X_train: ' + str(X_train.shape))\n",
    "    print('Y_train: ' + str(y_train.shape))\n",
    "    print('X_test:  '  + str(X_test.shape))\n",
    "    print('Y_test:  '  + str(y_test.shape))\n",
    "    \n",
    "    #Get indexes of a class to isolate in train set\n",
    "    class_index_train = np.where(np.isin(y_train, newclass)) #when newclass = 10, returns null\n",
    "    \n",
    "    #isolate the class from X and y, i.e create X and y without newclass\n",
    "    X_truncated = np.delete(X_train, class_index_train, 0)\n",
    "    y_truncated = np.delete(y_train, class_index_train)\n",
    "    df_input = pd.DataFrame(X_truncated)\n",
    "    df_input['y'] = y_truncated\n",
    "    \n",
    "    tree_multiplier = 1\n",
    "    n_classes=len(np.unique(y_truncated))\n",
    "    classes = np.unique(y_truncated)\n",
    "    print(\"Train Classes : \", classes)\n",
    "    print(\"New Class : \", newclass)\n",
    "    rootslist=[]\n",
    "    for i in range(n_classes):\n",
    "        #Since generative roots have no random factor when being fitted or when predicting. And since the lower levels of a tree don't affect the upper levels, it is sufficient to simply train the root nodes instead of the entire tree (and easier to fetch results).\n",
    "        node = Node(None, False, 1, 21, method_split='generative', root_class=i, alpha=0.99, root_distance=\"mahalanobis\")\n",
    "        node.fit(X_truncated, y_truncated)\n",
    "        rootslist.append(node)\n",
    "          \n",
    "    preds = np.zeros((n_classes, len(y_test)))\n",
    "    for root in rootslist:\n",
    "        root_class = root.root_class\n",
    "        pred = root.predict_splitting_function(X_test)\n",
    "        ones = np.where(np.isin(pred, 1))\n",
    "        preds[root_class, ones] = 1\n",
    "    \n",
    "        \n",
    "    f = open(\"test_HOG.txt\", \"a\")\n",
    "    for classtofind in range(len(classes)): #outputs reject rates for all classes, TODO: use y_pred vector\n",
    "        rejectlist=[]\n",
    "        #Get indexes of a class to isolate in test set\n",
    "        class_index_test = np.where(np.isin(y_test, classtofind))\n",
    "        print(\"\", file=f)\n",
    "        print(\"Class to find : \", classtofind, file=f)\n",
    "\n",
    "        for c in range(len(classes)):\n",
    "            reject = round(np.mean(preds[c, class_index_test][0] == 0), 3)*100\n",
    "            rejectlist.append(reject)\n",
    "            print(\"Combined rejection rate for samples of class {} by roots of class {} : {}\".format(classtofind, c, reject), file=f)\n",
    "\n",
    "        detection = 0\n",
    "        for index in range(len(rejectlist)): #template for class detection, currently if reject<50%, has no real effect on programm\n",
    "            if (rejectlist[index]<50):\n",
    "                print(\"Identified as class \", index)\n",
    "                detection = 1\n",
    "        if (detection==0):\n",
    "            print(\"New class detected !\")\n",
    "    f.close()    \n",
    "    print(\"See test_HOG.txt file in file system for results\")\n",
    "    \n",
    "if __name__ ==\"__main__\":\n",
    "    #Uncomment what you want to test\n",
    "    print(\"\")\n",
    "    #print(\"-----------GENERATIVE NODE----------\")\n",
    "    #test_gen_Node()\n",
    "    print(\"\")\n",
    "    #print(\"-----------Forest with pixel features----------\")\n",
    "    #test_forest()\n",
    "    print(\"\")\n",
    "    #print(\"-----------Forest with HOG----------\")\n",
    "    #HOG_test()\n",
    "    print(\"\")\n",
    "    #print(\"-----------New Class/outliers detection----------\")\n",
    "    #test_newclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712ef63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
